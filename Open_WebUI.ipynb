{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWOoE0c3XUzFVwKYvU/zGN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Axenide/Open-WebUI-Colab/blob/main/Open_WebUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open WebUI\n",
        "This is my first notebook so it is probably really messy. Anyways, I wanted to give people the opportunity to try Open WebUI without the need of better hardware.\n",
        "\n",
        "Check my [GitHub](https://github.com/Axenide) ðŸ‘ˆ\n",
        "\n",
        "## Getting the password for localtunnel\n",
        "In this section you will get your password for the localtunnel, which is basically the public IP for this Colab notebook.\n",
        "\n",
        "Run it.\n",
        "\n",
        "ðŸ‘‡"
      ],
      "metadata": {
        "id": "9AAJzmDMhE6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ],
      "metadata": {
        "id": "VRI9KMdTT1Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies\n",
        "Here we will install localtunnel and Ollama."
      ],
      "metadata": {
        "id": "cEdfep4Jhh6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "75u7Td2HV957"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting everything done\n",
        "This code will make Colab use Python 3.11, install Open WebUI via pip, run the Ollama server, pull Mistral Nemo and give the public URL where you need to enter the password you got in the first block of this notebook.\n",
        "\n",
        "Wait for the output to print \"success\" and you are done! If mistral-nemo isn't available, reload the page."
      ],
      "metadata": {
        "id": "AksDFR8miRaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JpgA4UIQnGA_"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3.11 python3.11-venv python3.11-dev\n",
        "\n",
        "# Create and activate a virtual environment using Python 3.11\n",
        "!python3.11 -m venv venv\n",
        "!source venv/bin/activate\n",
        "\n",
        "# Upgrade pip within the virtual environment\n",
        "!venv/bin/python -m pip install --upgrade pip\n",
        "\n",
        "# Install Open WebUI within the virtual environment\n",
        "!venv/bin/pip install open-webui\n",
        "\n",
        "# Create a script to start both servers asynchronously and expose them using localtunnel\n",
        "with open('start_servers.py', 'w') as f:\n",
        "    f.write('''\n",
        "import subprocess\n",
        "import threading\n",
        "import os\n",
        "import time\n",
        "\n",
        "def start_ollama():\n",
        "    subprocess.run(['ollama', 'serve'])\n",
        "\n",
        "def download_model():\n",
        "    subprocess.run(['ollama', 'pull', 'mistral-nemo'])\n",
        "\n",
        "def start_open_webui():\n",
        "    subprocess.run(['venv/bin/open-webui', 'serve', '--port', '8081'])\n",
        "\n",
        "# Start servers in separate threads\n",
        "threading.Thread(target=start_ollama).start()\n",
        "time.sleep(5)\n",
        "threading.Thread(target=download_model).start()\n",
        "threading.Thread(target=start_open_webui).start()\n",
        "''')\n",
        "\n",
        "# Execute the script\n",
        "!venv/bin/python start_servers.py && sleep 20 & npx localtunnel --port 8081\n"
      ]
    }
  ]
}